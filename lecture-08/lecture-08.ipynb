{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9539963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from scipy.stats import chisquare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916f53b2",
   "metadata": {},
   "source": [
    "# Lecture 8\n",
    "\n",
    "\n",
    "In this lecture, we are going to compare algorithms that aim to split a dataset into disjoint subsets. Given a collection of points $D = \\{ x_1,\\ldots,x_N \\}$ we would like to find a function $f\\colon D\\to \\{1,\\ldots,k\\}$. We are going to do this two different way: \n",
    "\n",
    "1. Supervised: \n",
    "    1. K-NN\n",
    "2. Unsupervised: \n",
    "    1. K-Means, \n",
    "    2. DBScan, \n",
    "    3. Hiearchical Clustering.\n",
    "\n",
    "The task is called *clustering* if the algorithm is unsupervised, and is called *classification* if the algorithm is supervised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2746f36",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "We are going to use a hyperspectral image dataset called [Indian Pine](https://www.kaggle.com/datasets/abhijeetgo/indian-pines-hyperspectral-dataset).\n",
    "\n",
    "The data is an image of size $145\\times 145$ where each pixel consists of 220 bands. Also, the pixels are labeled by numbers between $0$ to $16$ where $0$ is unclassified while other 16 correspond to certain crop types. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2664c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "pine = loadmat('./data/Indian_pines.mat')['indian_pines']\n",
    "pine_gt = loadmat('./data/Indian_pines_gt.mat')['indian_pines_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6625423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pine.reshape((145*145,220))\n",
    "y = pine_gt.reshape(145*145)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3124d2d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c3edc",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (K-NN)\n",
    "\n",
    "K-NN is a supervised algorithm which means our data set consists of pairs $(x^{(i)},y^{(i)})$ where $x^{(i)}\\in \\mathbb{R}^n$ and $y^{(i)}$ belongs to a finite set of labels. We are also given an odd number $k$.  If we are given a new point $x\\in\\mathbb{R}^n$ here is how we define $f(x)$:\n",
    "\n",
    "1. Find $k$-closest points $x^{(1)},\\ldots,x^{(k)}$ from the training set $D$ whose labels $y^{(1)},\\ldots,y^{(k)}$ are known.\n",
    "2. Find the majority label among $y^{(1)},\\ldots,y^{(k)}$ and return as $f(x)$.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c28157ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=7)\n",
    "model.fit(X_train,y_train)\n",
    "predicted = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "69d13957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.728552406315389"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5c5aa425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=288758.14228647517, pvalue=0.0)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chisquare(cm,axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1308899",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "K-Means is a unsupervised algorithm. We have data points $D = \\{ x^{(1)},\\ldots,x^{(N)} \\}$ and a number $k$. Here is a short description of the algorithm:\n",
    "\n",
    "1. Start with randomly chosen points $c^{(1)},\\ldots,c^{(k)}$ from $D$ and $k$ empty sets $S^{(1)},\\ldots,S^{(k)}$.\n",
    "2. For each point $x^{(i)}$ in $D$, find the closest centroid $c^{(j)}$ and add $x^{(i)}$ to $S^{(j)}$.\n",
    "3. Find the center of each $S^{(i)}$ and call them $d^{(1)},\\ldots,d^{(k)}$.\n",
    "4. If each $c^{(i)}$ and $d^{(i)}$ are close enough terminate the algorithm, if not assign $c^{(i)}\\leftarrow d^{(i)}$ and go to step 2.\n",
    "\n",
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4f6d1421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=165879.0181212842, pvalue=0.0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = len(np.unique(y))\n",
    "model = KMeans(n_clusters=k)\n",
    "predicted = model.fit_predict(X)\n",
    "cm = confusion_matrix(y,predicted)\n",
    "chisquare(cm,axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151e7840",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "Hierarchical clustering algorithm is an unsupervised algorithm. The algorithm gradually merge points into clusters. It does this by placing $B_\\epsilon(x)$ on each $x\\in D$. We merge two points $x_1$ and $x_2$ at a specific $\\epsilon$ if $B_\\epsilon(x_1)$ and $B_\\epsilon(x_2)$ intersect. While this is straightforward for individual pairs of points, one has to decide how clusters of points merge when $\\epsilon$ is large enough. This is called the *linkage* method. \n",
    "\n",
    "Let us use $ d_{(ij)k} $ to denote the distance between the clusters $ C_{k} $ and $ C_{ij} = C_i \\cup C_j $ which is merged in a single cluster.  One calculates the distance \n",
    "\\\\[ d_{(ij)k} = \\alpha_{ijk} d_{ik} +\\alpha_{jik} d_{jk}+ \\beta_{ijk} d_{ij} + \\gamma|d_{ik}-d_{jk}| \\\\] \n",
    "for parameters $\\alpha_{ijk}$, $\\beta_{ijk}$ and $\\gamma$ to be determined.  \n",
    "\n",
    "The most frequently used linkage methods are \n",
    "\n",
    "1. Single\n",
    "2. Complete\n",
    "3. Average\n",
    "4. Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79f0472",
   "metadata": {},
   "source": [
    "The parameters for commonly used methods of calculating distances between clusters.\n",
    "\n",
    "<table>\n",
    "    <tr><th style=\"width:100px\">Linkage</th>\n",
    "        <th style=\"width:100px\">$\\alpha_{ijk}$</th>\n",
    "        <th style=\"width:100px\">$\\beta_{ijk}$</th>\n",
    "        <th style=\"width:100px\">$\\gamma$</th></tr>\n",
    "    <tr><td>Single</td>\n",
    "        <td>$\\frac{1}{2}$</td>\n",
    "        <td>0</td>\n",
    "        <td>$-\\frac{1}{2}$</td></tr>\n",
    "    <tr><td>Complete</td>\n",
    "        <td>$\\frac{1}{2}$</td>\n",
    "        <td>0</td>\n",
    "        <td>$\\frac{1}{2}$</td></tr>\n",
    "    <tr><td>Average</td>\n",
    "        <td>$\\frac{n_i}{n_i+n_j}$</td>\n",
    "        <td>0</td>\n",
    "        <td>0</td></tr>\n",
    "    <tr><td>Ward</td>\n",
    "        <td>$\\frac{n_i+n_k}{n_i+n_j+n_k}$</td>\n",
    "        <td>$\\frac{-n_k}{n_i+n_j+n_k}$</td>\n",
    "        <td>0</td></tr>\n",
    "</table>\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8c96af3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=273278.3458739595, pvalue=0.0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = len(np.unique(y))\n",
    "model = AgglomerativeClustering(n_clusters=k, \n",
    "                                linkage=\"complete\", \n",
    "                                affinity='manhattan')\n",
    "predicted = model.fit_predict(X)\n",
    "chisquare(confusion_matrix(y,predicted),axis=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ded98d3",
   "metadata": {},
   "source": [
    "## DB-Scan\n",
    "\n",
    "DB-Scan is a unsupervised algorithm. So, we only have data points $x^{(1)},\\ldots,x^{(N)}$ and no output labels. The parameters are \n",
    "\n",
    "1. A real number $\\epsilon>0$, and \n",
    "2. A natural number $N$\n",
    "\n",
    "The key concepts for this algorithm are\n",
    "\n",
    "1. Core point\n",
    "2. Density connected points\n",
    "3. Density reachable points\n",
    "   1. Direct\n",
    "   2. Indirect\n",
    "4. Border points\n",
    "5. Outliers\n",
    "\n",
    "### Core points\n",
    "\n",
    "A point $x\\in D$ is called a *core point* if $B_\\epsilon(x)$ the ball with radius $\\epsilon$ centered at $x$ contains $N$ points from $D$.\n",
    "\n",
    "### Density connected points\n",
    "\n",
    "A pair of points $x$ and $y$ are called *density connected* if there is a core point $w$ such that $x,y\\in B_\\epsilon(w)$.\n",
    "\n",
    "### Density reachable points\n",
    "\n",
    "A point $x\\in D$ is called a *directly density reachable* if $x$ is lies inside $B_\\epsilon(u)$ the disk of radius $\\epsilon$ centered at a core point $u$. \n",
    "\n",
    "We call a point *indirectly density reachable* if there is a link of density connected points $x^{(1)},\\ldots,x^{(p)}$ i.e. $d(x^{(i)},x^{(i+1)})<\\epsilon$ for $i=1,\\ldots,p-1$ and $d(x^{(p)},x)<\\epsilon$.\n",
    "\n",
    "### Border points\n",
    "\n",
    "A point $x$ is called a *border point* if it is a density reachable point but $B_\\epsilon(x)$ has less than $N$ points from $D$.\n",
    "\n",
    "### Outlier points\n",
    "\n",
    "A point $x$ is called an outlier if $B_\\epsilon(x)$ contains less than $N$ points and $x$ is not a density reachable point.\n",
    "\n",
    "### The Algorithm\n",
    "\n",
    "We put point a collection of points $x^{(1)},\\ldots,x^{(\\ell)}$ in the same cluster if they are density reachable from each other.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8c370e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Power_divergenceResult(statistic=1967321.854696789, pvalue=0.0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DBSCAN()\n",
    "model.fit(X)\n",
    "predicted = model.fit_predict(X)\n",
    "chisquare(confusion_matrix(y,predicted),axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bb61f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
